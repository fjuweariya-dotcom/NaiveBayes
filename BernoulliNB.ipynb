{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a1ff8cd3-f9e0-4742-b7f3-bbb186fdaf04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bernoulli Naive Bayes Accuracy: 98.23%\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class BernoulliNaiveBayesFromScratch:\n",
    "    def fit(self, X, y):\n",
    "        self.classes = np.unique(y)\n",
    "        self.n_classes = len(self.classes)\n",
    "        self.n_features = X.shape[1]\n",
    "        self.class_priors = np.zeros(self.n_classes, dtype=np.float64)\n",
    "        # Probability of feature being 1 given class (P(X_i=1|y))\n",
    "        self.feature_prob_1 = np.zeros(\n",
    "            (self.n_classes, self.n_features), dtype=np.float64)\n",
    "        # Probability of feature being 0 given class (P(X_i=0|y))\n",
    "        self.feature_prob_0 = np.zeros(\n",
    "            (self.n_classes, self.n_features), dtype=np.float64)\n",
    "\n",
    "        # Laplace smoothing parameter (alpha)\n",
    "        alpha = 1.0\n",
    "\n",
    "        for i, c in enumerate(self.classes):\n",
    "            X_c = X[y == c]\n",
    "            self.class_priors[i] = X_c.shape[0] / X.shape[0]\n",
    "\n",
    "            for j in range(self.n_features):\n",
    "                # Calculate P(X_j=1 | y=c) using Laplace smoothing\n",
    "                count_1 = np.sum(X_c[:, j] == 1)\n",
    "                self.feature_prob_1[i, j] = (\n",
    "                    count_1 + alpha) / (X_c.shape[0] + 2 * alpha)\n",
    "                # Calculate P(X_j=0 | y=c) using Laplace smoothing\n",
    "                self.feature_prob_0[i, j] = 1.0 - self.feature_prob_1[i, j]\n",
    "\n",
    "    def predict(self, X):\n",
    "        predictions = [self._predict_sample(x) for x in X]\n",
    "        return np.array(predictions)\n",
    "\n",
    "    def _predict_sample(self, x):\n",
    "        # Calculate log probabilities to avoid underflow\n",
    "        posteriors = []\n",
    "\n",
    "        for i, c in enumerate(self.classes):\n",
    "            # Start with log prior\n",
    "            posterior = np.log(self.class_priors[i])\n",
    "\n",
    "            # Add log likelihood for each feature\n",
    "            for j in range(self.n_features):\n",
    "                if x[j] == 1:\n",
    "                    posterior += np.log(self.feature_prob_1[i, j])\n",
    "                else:\n",
    "                    posterior += np.log(self.feature_prob_0[i, j])\n",
    "            posteriors.append(posterior)\n",
    "\n",
    "        # Return the class with the highest log posterior probability\n",
    "        return self.classes[np.argmax(posteriors)]\n",
    "\n",
    "\n",
    "\n",
    "# --- Data Loading and Preprocessing ---\n",
    "# Load the dataset (assuming you have the data.csv file locally or can load it from a source)\n",
    "# You can download the dataset from the UCI Machine Learning Repository\n",
    "# or a source like Kaggle.\n",
    "# For this example, we assume the CSV is available as 'data.csv'.\n",
    "df = None  # Initialize df to None\n",
    "try:\n",
    "    df = pd.read_csv(r\"C:\\Users\\User\\OneDrive\\Desktop\\project semister 1\\breast cancer\\data.csv\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Dataset file not found. Please ensure 'data.csv' is in the directory.\")\n",
    "    # Use placeholder data or a method to fetch the data\n",
    "    # Example fetching (requires internet and a specific URL):\n",
    "    # url = \"archive.ics.uci.edu\"\n",
    "    # df = pd.read_csv(url, header=None)\n",
    "    # The columns will be different, so the subsequent code needs adjustment.\n",
    "    # No need to exit, as the subsequent 'if df is not None' will handle it\n",
    "\n",
    "# Only proceed if df was successfully loaded\n",
    "if df is not None:\n",
    "    # Create a copy for continuous features to avoid modifying the original df in place\n",
    "    df_continuous = df.copy()\n",
    "\n",
    "    # Drop unnecessary columns (e.g., ID and unnamed column) for both datasets\n",
    "    df_continuous = df_continuous.drop(['id', 'Unnamed: 32'], axis=1)\n",
    "    df = df.drop(['id', 'Unnamed: 32'], axis=1)\n",
    "\n",
    "    # Convert diagnosis to binary: 'M' (Malignant) = 1, 'B' (Benign) = 0 for both datasets\n",
    "    df_continuous['diagnosis'] = df_continuous['diagnosis'].map({\n",
    "                                                                'M': 1, 'B': 0})\n",
    "    df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})\n",
    "\n",
    "    # Binarize continuous features for Bernoulli NB: 1 if feature > mean, 0 otherwise\n",
    "    # This part remains for Bernoulli NB\n",
    "    for column in df.columns[1:]:  # Skip the 'diagnosis' column\n",
    "        mean_val = df[column].mean()\n",
    "        df[column] = (df[column] > mean_val).astype(int)\n",
    "\n",
    "    # Split data for Bernoulli NB into features (X) and target (y)\n",
    "    X = df.drop('diagnosis', axis=1).values\n",
    "    y = df['diagnosis'].values\n",
    "\n",
    "\n",
    "    # Split into training and testing sets (manual split as no sklearn is allowed)\n",
    "    def train_test_split_manual(X_data, y_data, test_size=0.2, random_state=42):\n",
    "        np.random.seed(random_state)\n",
    "        indices = np.arange(X_data.shape[0])\n",
    "        np.random.shuffle(indices)\n",
    "        test_samples = int(X_data.shape[0] * test_size)\n",
    "        test_indices = indices[:test_samples]\n",
    "        train_indices = indices[test_samples:]\n",
    "        X_train, X_test = X_data[train_indices], X_data[test_indices]\n",
    "        y_train, y_test = y_data[train_indices], y_data[test_indices]\n",
    "        return X_train, X_test, y_train, y_test\n",
    "\n",
    "    # For Bernoulli Naive Bayes\n",
    "    X_train_bnb, X_test_bnb, y_train_bnb, y_test_bnb = train_test_split_manual(\n",
    "        X, y, test_size=0.2)\n",
    "\n",
    "\n",
    "\n",
    "    # --- Train and Evaluate Bernoulli NB ---\n",
    "    bnb = BernoulliNaiveBayesFromScratch()\n",
    "    bnb.fit(X_train_bnb, y_train_bnb)\n",
    "    y_pred_bnb = bnb.predict(X_test_bnb)\n",
    "\n",
    "    # Calculate accuracy for Bernoulli NB\n",
    "    accuracy_bnb = np.sum(y_pred_bnb == y_test_bnb) / len(y_test_bnb)\n",
    "    print(f\"Bernoulli Naive Bayes Accuracy: {accuracy_bnb * 100:.2f}%\")\n",
    "\n",
    "else:\n",
    "    print(\"Cannot proceed with model training and evaluation due to missing 'data.csv'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dd22da6-ab76-4cbd-97e5-abb1b0d5a6ff",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
