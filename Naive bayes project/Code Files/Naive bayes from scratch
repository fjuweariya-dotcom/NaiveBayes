import time
import pandas as pd
import numpy as np
import hashlib

# --- Naive Bayes Classifier Implementations From Scratch ---

# --- HashTable Class Definition ---
# This defining hashTable class for Bernoulli, Gaussian Naive Bayes and Mulitinomial implementations
# to store likelihoods, means, and variances efficiently.


class HashTable:
    def __init__(self, size=1000):
        self.size = size
        self.table = [[] for _ in range(self.size)]

    def _get_hash_index(self, key):
        """Generates a hash index for a given key."""
        # Convert key to a string for consistent hashing, especially for tuples
        key_str = str(key)
        hash_obj = hashlib.sha256(key_str.encode('utf-8'))
        return int(hash_obj.hexdigest(), 16) % self.size

    def insert(self, key, value):
        """Inserts a key-value pair into the hash table."""
        index = self._get_hash_index(key)
        # Handle collisions by storing (key, value) pairs in a list at the index
        # First, remove existing key if present to update its value
        for i, (k, v) in enumerate(self.table[index]):
            if k == key:
                self.table[index][i] = (key, value)
                return
        self.table[index].append((key, value))

    def search(self, key):
        """Searches for a key and returns its associated value."""
        index = self._get_hash_index(key)
        for k, v in self.table[index]:
            if k == key:
                return v
        return None  # Key not found


class HashedNaiveBayes:
    def __init__(self, table_size=1000):
        self.table_size = table_size
        # Hash table to store counts: {class: [0] * table_size}
        self.feature_counts = defaultdict(lambda: [0] * self.table_size)
        self.class_counts = defaultdict(int)

    def _get_hash_index(self, word):
        """Map word to a table index using hashlib."""
        hash_obj = hashlib.sha256(word.encode('utf-8'))
        # Convert hex to int and modulo by table size
        return int(hash_obj.hexdigest(), 16) % self.table_size

    def train(self, features, label):
        self.class_counts[label] += 1
        for word in features:
            index = self._get_hash_index(word)
            self.feature_counts[label][index] += 1

    def predict(self, features):
        # Calculate P(Class|Features) using log-sums to avoid underflow
        # Implementation of Bayes Theorem: P(C|F) ∝ P(C) * ̲ P(Fi|C)
        pass


# --- Naive Bayes Classifier Implementations From Scratch ---


class HashTable:
    def __init__(self):
        self._storage = {}

    def insert(self, key, value):
        self._storage[key] = value

    def search(self, key):
        return self._storage.get(key, None)

    def __len__(self):
        return len(self._storage)

    def __str__(self):
        return str(self._storage)


class BernoulliNaiveBayesFromScratch:
    def __init__(self):
        # Initialize HashTable for likelihoods
        self._likelihood_table = HashTable()

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self._classes = np.unique(y)
        n_classes = len(self._classes)

        # Calculate P(class) - prior probabilities
        self._priors = np.zeros(n_classes)
        for idx, c in enumerate(self._classes):
            self._priors[idx] = np.sum(y == c) / n_samples

        # Calculate P(feature|class) - likelihoods using HashTable
        # Add Laplace smoothing to prevent zero probabilities
        alpha = 1  # Laplace smoothing parameter

        for idx, c in enumerate(self._classes):
            X_c = X[y == c]
            n_c = X_c.shape[0]

            # Calculate P(feature_j=1 | class_c) for all features
            p_feature_given_class_1 = (
                np.sum(X_c, axis=0) + alpha) / (n_c + alpha * 2)

            for j in range(n_features):
                # Store P(feature_j=1 | class_c) in hash table
                self._likelihood_table.insert(
                    (idx, j, 1), p_feature_given_class_1[j])
                # Store P(feature_j=0 | class_c) in hash table
                self._likelihood_table.insert(
                    (idx, j, 0), 1 - p_feature_given_class_1[j])

    def predict(self, X):
        return [self._predict(x) for x in X]

    def _predict(self, x):
        posteriors = []
        n_features = x.shape[0]

        # Calculate posterior probability for each class
        for idx, c_label in enumerate(self._classes):
            prior = np.log(self._priors[idx])

            class_conditional_log_likelihood = 0.0
            for j in range(n_features):
                feature_value = x[j]

                # Retrieve likelihood from HashTable
                if feature_value == 1:
                    likelihood = self._likelihood_table.search((idx, j, 1))
                else:  # feature_value == 0
                    likelihood = self._likelihood_table.search((idx, j, 0))

                # Add log likelihood to the sum
                class_conditional_log_likelihood += np.log(likelihood)

            posterior = prior + class_conditional_log_likelihood
            posteriors.append(posterior)

        return self._classes[np.argmax(posteriors)]


class GaussianNaiveBayesFromScratch:
    def __init__(self):
        # Initialize HashTable for means and variances
        self._mean_table = HashTable()
        self._variance_table = HashTable()

    def fit(self, X, y):
        n_samples, n_features = X.shape
        self._classes = np.unique(y)
        n_classes = len(self._classes)

        # Calculate prior for each class
        self._priors = np.zeros(n_classes)

        for idx, c in enumerate(self._classes):
            X_c = X[y == c]
            n_c = X_c.shape[0]
            self._priors[idx] = n_c / n_samples

            for j in range(n_features):
                # Calculate mean and variance for each feature in each class
                mean_val = np.mean(X_c[:, j])
                # Add a small epsilon to variance to prevent division by zero
                variance_val = np.var(X_c[:, j]) + 1e-9  # Add epsilon

                # Store mean and variance in hash tables
                self._mean_table.insert((idx, j), mean_val)
                self._variance_table.insert((idx, j), variance_val)

    def predict(self, X):
        return [self._predict(x) for x in X]

    def _predict(self, x):
        posteriors = []
        n_features = x.shape[0]

        # Calculate posterior probability for each class
        for idx, c in enumerate(self._classes):
            prior = np.log(self._priors[idx])
            # Calculate likelihood using Gaussian PDF: N(x; mu, sigma^2)
            class_conditional = np.sum(self._gaussian_pdf(idx, x, n_features))
            posterior = prior + class_conditional
            posteriors.append(posterior)

        return self._classes[np.argmax(posteriors)]

    def _gaussian_pdf(self, class_idx, x, n_features):
        log_likelihoods = []
        for j in range(n_features):
            mean = self._mean_table.search((class_idx, j))
            variance = self._variance_table.search((class_idx, j))

            if mean is None or variance is None:
                # Handle case where key might not be found (shouldn't happen with correct logic)
                # or if variance is 0 (after epsilon addition, it should not be)
                raise ValueError(
                    f"Mean or Variance not found for class_idx {class_idx}, feature_idx {j}")

            numerator = np.exp(- (x[j] - mean)**2 / (2 * variance))
            denominator = np.sqrt(2 * np.pi * variance)

            # Prevent log(0) if numerator becomes extremely small after calculation
            likelihood_val = numerator / denominator
            if likelihood_val == 0:  # Extremely small number, treat as very low log-likelihood
                # A very small number instead of 0
                log_likelihoods.append(np.log(1e-300))
            else:
                log_likelihoods.append(np.log(likelihood_val))

        # Return log likelihood to avoid underflow with very small numbers
        return np.array(log_likelihoods)


class MultinomialNaiveBayesFromScratch:
    def __init__(self, alpha=1.0):
        self.alpha = alpha  # Laplace smoothing parameter

    def fit(self, X, y):
        self.classes = np.unique(y)
        self.n_classes = len(self.classes)
        self.n_features = X.shape[1]

        self.class_priors = np.zeros(self.n_classes, dtype=np.float64)
        self.feature_counts = np.zeros(
            (self.n_classes, self.n_features), dtype=np.float64)
        self.class_totals = np.zeros(self.n_classes, dtype=np.float64)

        for i, c in enumerate(self.classes):
            X_c = X[y == c]
            self.class_priors[i] = X_c.shape[0] / X.shape[0]

            # Sum of all feature counts in this class
            self.feature_counts[i, :] = np.sum(X_c, axis=0)
            self.class_totals[i] = np.sum(self.feature_counts[i, :])

    def predict(self, X):
        predictions = [self._predict_sample(x) for x in X]
        return np.array(predictions)

    def _predict_sample(self, x):
        log_posteriors = []

        for i, c in enumerate(self.classes):
            log_prior = np.log(self.class_priors[i])
            log_likelihood = 0.0

            for j in range(self.n_features):
                # Calculate log probability with Laplace smoothing
                count = self.feature_counts[i, j]
                total = self.class_totals[i]
                prob = (count + self.alpha) / \
                    (total + self.n_features * self.alpha)

                # Multiply by feature value (assuming integer counts)
                if x[j] > 0:
                    log_likelihood += x[j] * np.log(prob)

            log_posteriors.append(log_prior + log_likelihood)

        return self.classes[np.argmax(log_posteriors)]


# --- Data Loading and Preprocessing --- (No changes)
# Load the dataset from 'breast_cancer.csv'
# Assuming 'data.csv' folder Breast cancer source as Kaggle
df = pd.read_csv(
    r"C:\Users\User\OneDrive\Desktop\All notes\Naive bayes project\breast cancer\data.csv")

# Drop unnecessary columns (e.g., ID and unnamed column from original dataset that are present)
# Check if these columns exist before dropping to avoid errors.
if 'id' in df.columns:
    df = df.drop(['id'], axis=1)
if 'Unnamed: 32' in df.columns:
    df = df.drop(['Unnamed: 32'], axis=1)

# Convert diagnosis to binary: 'M' (Malignant) = 1, 'B' (Benign) = 0
# The target column is often named 'diagnosis' and contains 'M'/'B' or 0/1 directly.
# If it's already 0/1, this mapping will not change it.
# If it's 'M'/'B', it will convert it.
# Check if diagnosis column is of object type (string)
if df['diagnosis'].dtype == 'object':
    df['diagnosis'] = df['diagnosis'].map({'M': 1, 'B': 0})

# Confirm the 'diagnosis' column contains binary values (0 and 1)
print("First 5 rows of the DataFrame:")
print(df.head())
print("\nValue counts for 'diagnosis' column:")
print(df['diagnosis'].value_counts())
print("First 5 rows of the DataFrame:")


# Create a copy for continuous features (for Gaussian NB) before binarization
df_continuous = df.copy()

# Binarize continuous features for Bernoulli NB: 1 if feature > mean, 0 otherwise
for column in df.columns[:-1]:  # Exclude the last column, which is 'diagnosis'
    mean_val = df[column].mean()
    df[column] = (df[column] > mean_val).astype(int)

# Split data for Bernoulli NB into features (X) and target (y)
X = df.drop('diagnosis', axis=1).values
y = df['diagnosis'].values

# Split data for Gaussian NB (without binarization) into features (X_continuous) and target (y_continuous)
X_continuous = df_continuous.drop('diagnosis', axis=1).values
y_continuous = df_continuous['diagnosis'].values

# Split into training and testing sets (manual split as no sklearn is allowed)


def train_test_split_manual(X_data, y_data, test_size=0.2, random_state=42):
    np.random.seed(random_state)
    indices = np.arange(X_data.shape[0])
    np.random.shuffle(indices)
    test_samples = int(X_data.shape[0] * test_size)
    test_indices = indices[:test_samples]
    train_indices = indices[test_samples:]
    X_train, X_test = X_data[train_indices], X_data[test_indices]
    y_train, y_test = y_data[train_indices], y_data[test_indices]
    return X_train, X_test, y_train, y_test


# For Bernoulli Naive Bayes
X_train_bnb, X_test_bnb, y_train_bnb, y_test_bnb = train_test_split_manual(
    X, y, test_size=0.2)

# For Gaussian Naive Bayes
X_train_gnb, X_test_gnb, y_train_gnb, y_test_gnb = train_test_split_manual(
    X_continuous, y_continuous, test_size=0.2)

# For Multinomial Naive Bayes (using binarized data X, y)
X_train_mnb, X_test_mnb, y_train_mnb, y_test_mnb = train_test_split_manual(
    X, y, test_size=0.2)

# --- Evaluation Metrics Helper Function ---


def calculate_precision_recall_f1(y_true, y_pred):
    # Ensure y_true and y_pred are numpy arrays for easier calculations
    y_true = np.array(y_true)
    y_pred = np.array(y_pred)

    # Assuming positive class is 1 and negative class is 0
    true_positives = np.sum((y_true == 1) & (y_pred == 1))
    false_positives = np.sum((y_true == 0) & (y_pred == 1))
    false_negatives = np.sum((y_true == 1) & (y_pred == 0))

    precision = true_positives / \
        (true_positives + false_positives) if (true_positives +
                                               false_positives) > 0 else 0
    recall = true_positives / \
        (true_positives + false_negatives) if (true_positives +
                                               false_negatives) > 0 else 0
    f1_score = (2 * precision * recall) / (precision +
                                           recall) if (precision + recall) > 0 else 0

    return precision, recall, f1_score


# --- Train and Evaluate Bernoulli NB ---
print("\n.1 Training Custom Bernoulli Naive Bayes...")

bnb = BernoulliNaiveBayesFromScratch()
start_time_bnb = time.time()
bnb.fit(X_train_bnb, y_train_bnb)
end_time_bnb = time.time()
training_time_bnb = end_time_bnb - start_time_bnb
y_pred_bnb = bnb.predict(X_test_bnb)

# Calculate accuracy for Bernoulli NB
accuracy_bnb = np.sum(y_pred_bnb == y_test_bnb) / len(y_test_bnb)
print(
    f"Bernoulli Naive Bayes Accuracy (with HashTable): {accuracy_bnb * 100:.2f}%")
print(f"Bernoulli Naive Bayes Training Time: {training_time_bnb:.4f} seconds")

# Calculate Precision, Recall, F1 for Bernoulli NB
precision_bnb, recall_bnb, f1_bnb = calculate_precision_recall_f1(
    y_test_bnb, y_pred_bnb)
print(f"Bernoulli Naive Bayes Precision: {precision_bnb * 100:.2f}%")
print(f"Bernoulli Naive Bayes Recall: {recall_bnb * 100:.2f}%")
print(f"Bernoulli Naive Bayes F1 Score: {f1_bnb * 100:.2f}%")

# --- Train and Evaluate Gaussian NB ---
print("\n2. Training Custom Gaussian Naive Bayes...")
gnb = GaussianNaiveBayesFromScratch()
start_time_gnb = time.time()
gnb.fit(X_train_gnb, y_train_gnb)
end_time_gnb = time.time()
training_time_gnb = end_time_gnb - start_time_gnb
y_pred_gnb = gnb.predict(X_test_gnb)

# Calculate accuracy for Gaussian NB
accuracy_gnb = np.sum(y_pred_gnb == y_test_gnb) / len(y_test_gnb)
print(
    f"Gaussian Naive Bayes Accuracy (with HashTable): {accuracy_gnb * 100:.2f}%")
print(f"Gaussian Naive Bayes Training Time: {training_time_gnb:.4f} seconds")

# Calculate Precision, Recall, F1 for Gaussian NB
precision_gnb, recall_gnb, f1_gnb = calculate_precision_recall_f1(
    y_test_gnb, y_pred_gnb)
print(f"Gaussian Naive Bayes Precision: {precision_gnb * 100:.2f}%")
print(f"Gaussian Naive Bayes Recall: {recall_gnb * 100:.2f}%")
print(f"Gaussian Naive Bayes F1 Score: {f1_gnb * 100:.2f}%")

# Train custom Multinomial Naive Bayes
print("\n3. Training Custom Multinomial Naive Bayes...")
start_time = time.time()
mnb_custom = MultinomialNaiveBayesFromScratch(alpha=1.0)
mnb_custom.fit(X_train_mnb, y_train_mnb)
custom_train_time = time.time() - start_time
print(f"   Training time: {custom_train_time:.4f} seconds")
y_pred_mnb = mnb_custom.predict(X_test_mnb)

# Calculate accuracy for Multinomial NB
accuracy_mnb = np.sum(y_pred_mnb == y_test_mnb) / len(y_test_mnb)
print(
    f"Multinomial Naive Bayes Accuracy (with HashTable): {accuracy_mnb * 100:.2f}%")
print(
    f"Multinomial Naive Bayes Training Time: {custom_train_time:.4f} seconds")

# Calculate Precision, Recall, F1 for Multinomial NB
precision_mnb, recall_mnb, f1_mnb = calculate_precision_recall_f1(
    y_test_mnb, y_pred_mnb)
print(f"Multinomial Naive Bayes Precision: {precision_mnb * 100:.2f}%")
print(f"Multinomial Naive Bayes Recall: {recall_mnb * 100:.2f}%")
print(f"Multinomial Naive Bayes F1 Score: {f1_mnb * 100:.2f}%")
